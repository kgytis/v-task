{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program written in Python 3.11.1 using Jupiter Notebook.\n",
    "\n",
    "To run program, execute cell below. (data file directory structures should be the same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapper function responsible of generating key-alue dictionary pairs and returning them\n",
    "def mapper_task_1(data):\n",
    "    results = []\n",
    "    for click in data:\n",
    "        key = click['date']\n",
    "        value = click\n",
    "        results.append(({'key': key, 'value': value}))\n",
    "    return results\n",
    "\n",
    "# Reducer funtion responsible for count, how many each day there were clicks\n",
    "def reducer_task_1(key, values):\n",
    "    result = []\n",
    "    result.append({'date': key, 'count': len(values)})\n",
    "    # Returning all key-value pairs\n",
    "    return result\n",
    "\n",
    "# CUSTOM mapReduce IMPLEMENTATION\n",
    "# As number of datasets differs in both tasks, mapReduce takes list of datasets and list of mappers (if only one should be passed, it should be in a list as well)\n",
    "def map_reduce(datasets, mappers, reducer):\n",
    "    # Apply the mapper to the datasets\n",
    "    intermediate_data = []\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        mapper = mappers[i]\n",
    "        intermediate_data += mapper(dataset)\n",
    "    # Group the intermediate data by key\n",
    "    grouped_data = {}\n",
    "    for item in intermediate_data:\n",
    "        if item['key'] in grouped_data:\n",
    "            grouped_data[item['key']].append(item['value'])\n",
    "        else:\n",
    "            grouped_data[item['key']] = [item['value']]\n",
    "    # Apply the reducer to each group of values\n",
    "    final_data = []\n",
    "    for key, values in grouped_data.items():\n",
    "        final_data.extend(reducer(key, values))\n",
    "    # Return the final output\n",
    "    return final_data\n",
    "\n",
    "# TWO MAP FUNCTIONS FOR SECOND TASK\n",
    "# Same return as mapper function in first task, just additional implementation of table key value pair\n",
    "# 'table' : 'users' <-- for users dataset\n",
    "# 'table' : 'clicks' <-- for clicks dataset\n",
    "def mapper_task_2_users(users):\n",
    "    results = []\n",
    "    for user in users:\n",
    "        if user['country'] == 'LT':\n",
    "            key = user['id']\n",
    "            value = user.copy()\n",
    "            # indicator that this is a users table\n",
    "            value['table'] = 'users'\n",
    "            results.append({'key': key, 'value': value})\n",
    "    return results\n",
    "\n",
    "def mapper_task_2_clicks(clicks):\n",
    "    results = []\n",
    "    for click in clicks:\n",
    "        key = click['user_id']\n",
    "        value = click\n",
    "        # indicator that this is a users table\n",
    "        value['table'] = 'clicks'\n",
    "        results.append(({'key': key, 'value': value}))\n",
    "    return results\n",
    "\n",
    "# Reducer accepts mapped data (LT user + his clicks) + unmatched clicks, which are later filtered out\n",
    "def reducer_task_2(key, values):\n",
    "    # Values -> LT user : his click | LT user : no clicks | clicks without LT user (first mapper filters out users that are not from LT)\n",
    "    # user -> Filters out from values onlt user data (needed to filter out clicks)\n",
    "    user = next((value for value in values if value['table'] == 'users'), None)\n",
    "    filtered_clicks = []\n",
    "    for click in values:\n",
    "        # From grouped dataset takes only data that are clicks\n",
    "        # And merges user data with click data (like join) (user data + click data) --> single line\n",
    "        # After that it appends all merged clicks to filtered list (user data + click data) --> multiple lines\n",
    "        if click['table'] == 'clicks':\n",
    "            merged_click = click.copy()\n",
    "            if user is not None:\n",
    "                merged_click.update(user)\n",
    "            if 'country' in merged_click and merged_click['country'] is not None:\n",
    "                filtered_clicks.append(merged_click)\n",
    "    return filtered_clicks\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "\n",
    "dir_path_clicks = f'{os.getcwd()}/data/clicks'\n",
    "file_path_clicks = 'data/clicks'\n",
    "\n",
    "dir_path_users = f'{os.getcwd()}/data/users'\n",
    "file_path_users = 'data/users'\n",
    "\n",
    "# File reader function to read files from data source\n",
    "def file_reader(dir_path, file_path):\n",
    "    # Loop responsible for opening files\n",
    "    data_set = []\n",
    "    for file in os.listdir(dir_path):\n",
    "        data = open(f'{file_path}/{file}', encoding='utf-8')\n",
    "        csv_data = list(csv.reader(data))\n",
    "        # Below loop applies key to every csv file cell. That data for both datasets would be homogenious\n",
    "        headers = csv_data[0]\n",
    "        for data_row in csv_data[1:]:\n",
    "            my_dict = {}\n",
    "            for i, cell in enumerate(data_row):\n",
    "                my_dict[headers[i]] = cell\n",
    "            data_set.append(my_dict)\n",
    "    data.close()\n",
    "    return data_set\n",
    "\n",
    "\n",
    "data_set_clicks = file_reader(dir_path_clicks, file_path_clicks)\n",
    "data_set_users = file_reader(dir_path_users, file_path_users)\n",
    "\n",
    "final_data_1 = map_reduce(datasets=[data_set_clicks], mappers=[mapper_task_1], reducer=reducer_task_1)\n",
    "final_data_2 = map_reduce(datasets=[data_set_users, data_set_clicks], mappers=[mapper_task_2_users, mapper_task_2_clicks], reducer=reducer_task_2)\n",
    "\n",
    "\n",
    "with open('data/filtered_clicks', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['date','user_id', 'screen', 'click_target','table','id','country'])\n",
    "    for data in final_data_2:\n",
    "        if 'screen' not in data:\n",
    "            row = [data['date'], data['user_id'], 'null', data['click_target'], data['table'], data['id'], data['country']]\n",
    "        else:\n",
    "            row = [data['date'], data['user_id'], data['screen'],data['click_target'], data['table'], data['id'], data['country']]\n",
    "        writer.writerow(row)\n",
    "\n",
    "with open('data/total_clicks', 'w', encoding='UTF8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['date', 'count'])\n",
    "    for data in final_data_1:\n",
    "        row = [data['date'], data['count']]        \n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
